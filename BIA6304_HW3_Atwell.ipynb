{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Mike Atwell\n",
    "####Rockhurst University\n",
    "####BIA6304\n",
    "####2018 Fall Term 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Package imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import all of the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150) #important for getting all the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 is to \"read in or create a data frame with at least one column of text to be analyzed.  This could be the text you used previously or new text. Choose a prediction you would like to make with these data and create the appropriate feature space. Identify the labels you will be trying to predict and proceed to create a train-test split. Using default model parameters, fit 3 classifiers (decision tree, naïve bayes, logistic regression, or knn) to your dataset and subsequently generate predictions (just like we did in class). Feel free to set a random state variable where appropriate to facilitate replication.  Assess the performance of the models using any of the measures (confusion matrices, precision, recall, f1-score, and accuracy).\"  \n",
    "\n",
    "For this purpose I used a dataset of text messages and a classification for each as 'spam' or 'ham' available here: \n",
    "\n",
    "https://www.kaggle.com/team-ai/spam-text-message-classification\n",
    "\n",
    "The label I'll be trying to predict is whether a message is spam or not, and is already contained in the dataset.  Let's import it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Import and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "0         ham   \n",
       "1         ham   \n",
       "2        spam   \n",
       "3         ham   \n",
       "4         ham   \n",
       "...       ...   \n",
       "5567     spam   \n",
       "5568      ham   \n",
       "5569      ham   \n",
       "5570      ham   \n",
       "5571      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \n",
       "0                                           Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                             Ok lar... Joking wif u oni...  \n",
       "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007...  \n",
       "3                                                                                                         U dun say so early hor... U c already then say...  \n",
       "4                                                                                             Nah I don't think he goes to usf, he lives around here though  \n",
       "...                                                                                                                                                     ...  \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...  \n",
       "5568                                                                                                                   Will ü b going to esplanade fr home?  \n",
       "5569                                                                                              Pity, * was in mood for that. So...any other suggestions?  \n",
       "5570                          The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free  \n",
       "5571                                                                                                                             Rofl. Its true to its name  \n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10) #sets output to desired length\n",
    "spam_data = pd.read_csv(\"SPAM text message 20170820 - Data.csv\") #imports csv\n",
    "spam_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data has 5,572 rows and two columns, the 'spam' or 'ham' identifier and the text message itslef.  Before conducting the task, let's make sure the dataset is ready.  First, a quick check for nulls in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls in the dataset.  Now let's check to ensure that there are no other categories besides ham or spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.Category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category column doesn't contain any values other than 'ham' or 'spam', and we see about 13.4% of the messages are spam.  \n",
    "\n",
    "The dataset appears ready for this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preparing dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get the count vectorizer and conduct a test/train split so we can run the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8709 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0      0    0       0             0     0     0            0            0   \n",
       "1      0    0       0             0     0     0            0            0   \n",
       "2      0    0       0             0     0     0            0            0   \n",
       "3      0    0       0             0     0     0            0            0   \n",
       "4      0    0       0             0     0     0            0            0   \n",
       "...   ..  ...     ...           ...   ...   ...          ...          ...   \n",
       "5567   0    0       0             0     0     0            0            0   \n",
       "5568   0    0       0             0     0     0            0            0   \n",
       "5569   0    0       0             0     0     0            0            0   \n",
       "5570   0    0       0             0     0     0            0            0   \n",
       "5571   0    0       0             0     0     0            0            0   \n",
       "\n",
       "      0125698789  02 ...   zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \\\n",
       "0              0   0 ...       0       0    0          0     0     0      0   \n",
       "1              0   0 ...       0       0    0          0     0     0      0   \n",
       "2              0   0 ...       0       0    0          0     0     0      0   \n",
       "3              0   0 ...       0       0    0          0     0     0      0   \n",
       "4              0   0 ...       0       0    0          0     0     0      0   \n",
       "...          ...  .. ...     ...     ...  ...        ...   ...   ...    ...   \n",
       "5567           0   0 ...       0       0    0          0     0     0      0   \n",
       "5568           0   0 ...       0       0    0          0     0     0      0   \n",
       "5569           0   0 ...       0       0    0          0     0     0      0   \n",
       "5570           0   0 ...       0       0    0          0     0     0      0   \n",
       "5571           0   0 ...       0       0    0          0     0     0      0   \n",
       "\n",
       "      èn  ú1  〨ud  \n",
       "0      0   0    0  \n",
       "1      0   0    0  \n",
       "2      0   0    0  \n",
       "3      0   0    0  \n",
       "4      0   0    0  \n",
       "...   ..  ..  ...  \n",
       "5567   0   0    0  \n",
       "5568   0   0    0  \n",
       "5569   0   0    0  \n",
       "5570   0   0    0  \n",
       "5571   0   0    0  \n",
       "\n",
       "[5572 rows x 8709 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textStr = spam_data['Message'] #creates a generic name for the headline text; useful for reusing the code for other projects \n",
    "\n",
    "cv1 = CountVectorizer(binary=False, lowercase=True) \n",
    "#thought that mutiple uses of the same words might give the model more predictive power, thus kept binary to False\n",
    "\n",
    "cv1_chat = cv1.fit_transform(textStr) #transforms text\n",
    "\n",
    "word_count = pd.DataFrame(cv1_chat.toarray(),columns = cv1.get_feature_names())\n",
    "\n",
    "pd.set_option('display.max_rows', 10) #sets output to desired length\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the feature space is 8,709 unique words or numbers.  Now let's split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 8709)\n",
      "(1672, 8709)\n",
      "(3900,)\n",
      "(1672,)\n"
     ]
    }
   ],
   "source": [
    "y = spam_data['Category'].values #this is an array of labels\n",
    "X = word_count\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2112) #random_state is set seed\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a training set of 3,900 rows and a test set of 1,672 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the models on the data.  For our initial look we'll use three with default settings: Decision Tree, Naive Bayes, and Logisitic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=2112,\n",
      "            splitter='best')\n",
      "0.9706937799043063\n",
      "accuracy: 0.9706937799043063\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.99      0.98      1450\n",
      "       spam       0.93      0.84      0.88       222\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1436   14]\n",
      " [  35  187]]\n"
     ]
    }
   ],
   "source": [
    "# fit a Decision Tree model_dt1 to the data\n",
    "model_dt1 = DecisionTreeClassifier(random_state = 2112)\n",
    "print(model_dt1)\n",
    "model_dt1.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf1_expected = y_test\n",
    "clf1_predicted = model_dt1.predict(X_test)\n",
    "\n",
    "print(model_dt1.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_dt1\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf1_expected, clf1_predicted)))\n",
    "print(metrics.classification_report(clf1_expected, clf1_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf1_expected, clf1_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree yielded an acuracy of 0.971.  Now let's try the Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.9808612440191388\n",
      "accuracy: 0.9808612440191388\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      1450\n",
      "       spam       0.92      0.94      0.93       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1432   18]\n",
      " [  14  208]]\n"
     ]
    }
   ],
   "source": [
    "# fit a Naive Bayes model_nb1 to the data\n",
    "model_nb1 = MultinomialNB()\n",
    "print(model_nb1)\n",
    "model_nb1.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf2_expected = y_test\n",
    "clf2_predicted = model_nb1.predict(X_test)\n",
    "\n",
    "print(model_nb1.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_nb1\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf2_expected, clf2_predicted)))\n",
    "print(metrics.classification_report(clf2_expected, clf2_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf2_expected, clf2_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes had an accuracy of 0.981, slightly better than the Decision Tree.  Now let's try logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=2112, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9820574162679426\n",
      "accuracy: 0.9820574162679426\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99      1450\n",
      "       spam       0.98      0.88      0.93       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1446    4]\n",
      " [  26  196]]\n"
     ]
    }
   ],
   "source": [
    "# fit a logistic regression model_lr1 to the data\n",
    "model_lr1 = LogisticRegression(random_state = 2112)\n",
    "print(model_lr1)\n",
    "model_lr1.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf3_expected = y_test\n",
    "clf3_predicted = model_lr1.predict(X_test)\n",
    "\n",
    "print(model_lr1.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_lr1\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf3_expected, clf3_predicted)))\n",
    "print(metrics.classification_report(clf3_expected, clf3_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf3_expected, clf3_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression had even better accuracy at 0.982."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Model Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier comparison of model performance, let's list the measures side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>1436</td>\n",
       "      <td>187</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>0.970694</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.842342</td>\n",
       "      <td>0.884161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>1432</td>\n",
       "      <td>208</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.936937</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>1446</td>\n",
       "      <td>196</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982057</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.928910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TN   TP  FN  FP  Accuracy  Precision    Recall  \\\n",
       "Model                                                                   \n",
       "Decision Tree        1436  187  35  14  0.970694   0.930348  0.842342   \n",
       "Naive Bayes          1432  208  14  18  0.980861   0.920354  0.936937   \n",
       "Logistic Regression  1446  196  26   4  0.982057   0.980000  0.882883   \n",
       "\n",
       "                     F1 Score  \n",
       "Model                          \n",
       "Decision Tree        0.884161  \n",
       "Naive Bayes          0.928571  \n",
       "Logistic Regression  0.928910  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = {'Model': ['Decision Tree',\n",
    "                          'Naive Bayes',\n",
    "                          'Logistic Regression'],\n",
    "                'TN': [1436, 1432, 1446],\n",
    "                'TP': [187,208,196],\n",
    "                'FN' : [35,14,26],\n",
    "                'FP' : [14,18,4]}\n",
    "#captures data determined above \n",
    "#TN = True Negative\n",
    "#TP = True Positive\n",
    "#FN = False Negative\n",
    "#FP = False Positive\n",
    "\n",
    "model_metrics = pd.DataFrame(data=metrics_data) #creates dataframe\n",
    "model_metrics = model_metrics.set_index('Model') #uses model name as row title\n",
    "total = model_metrics['TN']+model_metrics['TP']+model_metrics['FN']+model_metrics['FP']\n",
    "model_metrics['Accuracy'] = (model_metrics['TN'] + model_metrics['TP'])/total #calculates accuracy\n",
    "model_metrics['Precision'] = model_metrics['TP']/(model_metrics['TP']+model_metrics['FP']) #calculates precision\n",
    "model_metrics['Recall'] = model_metrics['TP']/(model_metrics['TP']+model_metrics['FN']) #calculates recall\n",
    "model_metrics['F1 Score'] = 2*model_metrics['Precision']*model_metrics['Recall']/(model_metrics['Precision']+model_metrics['Recall']) #calculates F1 score\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the logisitic regression performed the best on all but one measure, recall, since it at a higher number of false negatives than the naive bayes.  \n",
    "\n",
    "In this business case I judge the logisitic regression the best, becuase of the two undesired possibilities (false positive or false negative), I consider false negatives to be more tolerable than false positives; thus, precision is the most important metric.  In essence, false negatives are spam that get through the filter, whereas false positives are real messages that get filtered out.  My preference is to deal with a bit more spam than to have real messages get filtered out.  The logistic regression was far superior to the other models in this regard, with a precision of .98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see about tweaking the models to squeeze out better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 is \"using a variety of parameter settings (for classifiers or vectorizers), try to improve on the performance of the baseline models.  At least 6 separate predictions should be run and the results reported in a table.  You can use any combination of parameters and classifiers; you do not need to use all classifiers. Make sure at least one example uses a preprocessing option (stemming, lemmatization, custom dictionary, custom stopwords, etc.)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll tweak the decision tree.  We'll start with a grid search on the number of max features to determine the best value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_features': 11}\n",
      "Best Score: 0.9479487179487179\n",
      "Best Estimator DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=11, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=2112,\n",
      "            splitter='best')\n",
      "Grid Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.690875</td>\n",
       "      <td>0.063487</td>\n",
       "      <td>0.037457</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_features': 3}</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944359</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555892</td>\n",
       "      <td>0.040733</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_features': 4}</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.956410</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944359</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_features': 5}</td>\n",
       "      <td>0.941026</td>\n",
       "      <td>0.937179</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941026</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.508545</td>\n",
       "      <td>0.035423</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_features': 6}</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>0.951282</td>\n",
       "      <td>0.944872</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943846</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.452669</td>\n",
       "      <td>0.006519</td>\n",
       "      <td>0.020021</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_features': 7}</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>0.932051</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935128</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.466926</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_features': 8}</td>\n",
       "      <td>0.937179</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>0.958974</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946410</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.463971</td>\n",
       "      <td>0.008359</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_features': 9}</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>0.947436</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938718</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.470175</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.017037</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_features': 10}</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940513</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.464358</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_features': 11}</td>\n",
       "      <td>0.951282</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.947436</td>\n",
       "      <td>0.951282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947949</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.543365</td>\n",
       "      <td>0.053366</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>0.007058</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_features': 12}</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.956410</td>\n",
       "      <td>0.937179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946410</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.525998</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>13</td>\n",
       "      <td>{'max_features': 13}</td>\n",
       "      <td>0.937179</td>\n",
       "      <td>0.928205</td>\n",
       "      <td>0.952564</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.942051</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.452797</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>14</td>\n",
       "      <td>{'max_features': 14}</td>\n",
       "      <td>0.926923</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.941026</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937179</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.690875      0.063487         0.037457        0.006287   \n",
       "1        0.555892      0.040733         0.035587        0.003454   \n",
       "2        0.559700      0.023143         0.037708        0.009486   \n",
       "3        0.508545      0.035423         0.024089        0.003825   \n",
       "4        0.452669      0.006519         0.020021        0.005296   \n",
       "5        0.466926      0.014048         0.018139        0.003477   \n",
       "6        0.463971      0.008359         0.016833        0.000201   \n",
       "7        0.470175      0.007165         0.017037        0.000243   \n",
       "8        0.464358      0.006633         0.017000        0.000095   \n",
       "9        0.543365      0.053366         0.023646        0.007058   \n",
       "10       0.525998      0.030006         0.022472        0.006901   \n",
       "11       0.452797      0.008897         0.016065        0.000525   \n",
       "\n",
       "   param_max_features                params  split0_test_score  \\\n",
       "0                   3   {'max_features': 3}           0.933333   \n",
       "1                   4   {'max_features': 4}           0.942308   \n",
       "2                   5   {'max_features': 5}           0.941026   \n",
       "3                   6   {'max_features': 6}           0.943590   \n",
       "4                   7   {'max_features': 7}           0.939744   \n",
       "5                   8   {'max_features': 8}           0.937179   \n",
       "6                   9   {'max_features': 9}           0.938462   \n",
       "7                  10  {'max_features': 10}           0.933333   \n",
       "8                  11  {'max_features': 11}           0.951282   \n",
       "9                  12  {'max_features': 12}           0.939744   \n",
       "10                 13  {'max_features': 13}           0.937179   \n",
       "11                 14  {'max_features': 14}           0.926923   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score       ...         \\\n",
       "0            0.948718           0.958974           0.933333       ...          \n",
       "1            0.942308           0.956410           0.943590       ...          \n",
       "2            0.937179           0.955128           0.939744       ...          \n",
       "3            0.951282           0.944872           0.925641       ...          \n",
       "4            0.932051           0.930769           0.938462       ...          \n",
       "5            0.943590           0.958974           0.935897       ...          \n",
       "6            0.939744           0.947436           0.925641       ...          \n",
       "7            0.929487           0.950000           0.933333       ...          \n",
       "8            0.948718           0.947436           0.951282       ...          \n",
       "9            0.948718           0.956410           0.937179       ...          \n",
       "10           0.928205           0.952564           0.948718       ...          \n",
       "11           0.942308           0.941026           0.933333       ...          \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.944359        0.009851                4                 1.0   \n",
       "1          0.944359        0.006415                4                 1.0   \n",
       "2          0.941026        0.007692                8                 1.0   \n",
       "3          0.943846        0.009878                6                 1.0   \n",
       "4          0.935128        0.003497               12                 1.0   \n",
       "5          0.946410        0.009608                2                 1.0   \n",
       "6          0.938718        0.007225               10                 1.0   \n",
       "7          0.940513        0.010653                9                 1.0   \n",
       "8          0.947949        0.003768                1                 1.0   \n",
       "9          0.946410        0.007041                2                 1.0   \n",
       "10         0.942051        0.008635                7                 1.0   \n",
       "11         0.937179        0.006122               11                 1.0   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0                  1.0                 1.0                 1.0   \n",
       "1                  1.0                 1.0                 1.0   \n",
       "2                  1.0                 1.0                 1.0   \n",
       "3                  1.0                 1.0                 1.0   \n",
       "4                  1.0                 1.0                 1.0   \n",
       "5                  1.0                 1.0                 1.0   \n",
       "6                  1.0                 1.0                 1.0   \n",
       "7                  1.0                 1.0                 1.0   \n",
       "8                  1.0                 1.0                 1.0   \n",
       "9                  1.0                 1.0                 1.0   \n",
       "10                 1.0                 1.0                 1.0   \n",
       "11                 1.0                 1.0                 1.0   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0                  1.0               1.0              0.0  \n",
       "1                  1.0               1.0              0.0  \n",
       "2                  1.0               1.0              0.0  \n",
       "3                  1.0               1.0              0.0  \n",
       "4                  1.0               1.0              0.0  \n",
       "5                  1.0               1.0              0.0  \n",
       "6                  1.0               1.0              0.0  \n",
       "7                  1.0               1.0              0.0  \n",
       "8                  1.0               1.0              0.0  \n",
       "9                  1.0               1.0              0.0  \n",
       "10                 1.0               1.0              0.0  \n",
       "11                 1.0               1.0              0.0  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a full grid over several parameters and cross validate 5 times\n",
    "pd.set_option('display.max_rows', 15) #sets output to desired length\n",
    "param_grid = {\"max_features\": [3,4,5,6,7,8,9,10,11,12,13,14]}\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_dt1, param_grid=param_grid,n_jobs=-1,cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_search.best_params_) \n",
    "print(\"Best Score:\", grid_search.best_score_) \n",
    "print(\"Best Estimator\", grid_search.best_estimator_) \n",
    "print(\"Grid Scores:\")\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search found that a max features of 11 was optimal.  Let's put it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=11, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=2112,\n",
      "            splitter='best')\n",
      "0.9623205741626795\n",
      "accuracy: 0.9623205741626795\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      0.99      0.98      1450\n",
      "       spam       0.94      0.76      0.84       222\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1440   10]\n",
      " [  53  169]]\n"
     ]
    }
   ],
   "source": [
    "# fit a Decision Tree model_dt2 to the data\n",
    "model_dt2 = DecisionTreeClassifier(max_features = 11, random_state = 2112)\n",
    "print(model_dt2)\n",
    "model_dt2.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf7_expected = y_test\n",
    "clf7_predicted = model_dt2.predict(X_test)\n",
    "\n",
    "print(model_dt2.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_dt2\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf7_expected, clf7_predicted)))\n",
    "print(metrics.classification_report(clf7_expected, clf7_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf7_expected, clf7_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweak improved the number of false positives, dropping from 14 to 10, but the number of false negatives increased from 35 to 53.  Though this decreases overall accuracy slightly, I consider this an acceptable trade off since false positives (real texts that are filtered out) are the worst possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try tweaking the Naive Bayes.  We'll do a grid search on alpha, the smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'alpha': 0.01}\n",
      "Best Score: 0.9805128205128205\n",
      "Best Estimator MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Grid Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.632115</td>\n",
       "      <td>0.045101</td>\n",
       "      <td>0.050448</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>{'alpha': 1e-06}</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.970513</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977692</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>7</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.996987</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.613954</td>\n",
       "      <td>0.047466</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'alpha': 1e-05}</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.971795</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.975641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978462</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.996987</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.591759</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'alpha': 0.0001}</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977949</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>6</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.597808</td>\n",
       "      <td>0.040917</td>\n",
       "      <td>0.056444</td>\n",
       "      <td>0.064473</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978974</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>4</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.996603</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587120</td>\n",
       "      <td>0.046640</td>\n",
       "      <td>0.025111</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.975641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980513</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.996218</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.610113</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.028788</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980513</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994551</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>0.995192</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>0.995321</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.607519</td>\n",
       "      <td>0.062348</td>\n",
       "      <td>0.037345</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>3</td>\n",
       "      <td>0.993269</td>\n",
       "      <td>0.993910</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.992949</td>\n",
       "      <td>0.994231</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.637259</td>\n",
       "      <td>0.074892</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>5</td>\n",
       "      <td>{'alpha': 5}</td>\n",
       "      <td>0.962821</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964359</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>8</td>\n",
       "      <td>0.973397</td>\n",
       "      <td>0.972436</td>\n",
       "      <td>0.971795</td>\n",
       "      <td>0.974679</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.973462</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.613111</td>\n",
       "      <td>0.072002</td>\n",
       "      <td>0.031069</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>0.947436</td>\n",
       "      <td>0.939744</td>\n",
       "      <td>0.951282</td>\n",
       "      <td>0.943590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>9</td>\n",
       "      <td>0.952244</td>\n",
       "      <td>0.955449</td>\n",
       "      <td>0.950962</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.953205</td>\n",
       "      <td>0.953141</td>\n",
       "      <td>0.001509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.632115      0.045101         0.050448        0.002526       1e-06   \n",
       "1       0.613954      0.047466         0.039683        0.010306       1e-05   \n",
       "2       0.591759      0.023623         0.024601        0.000471      0.0001   \n",
       "3       0.597808      0.040917         0.056444        0.064473       0.001   \n",
       "4       0.587120      0.046640         0.025111        0.001142        0.01   \n",
       "5       0.610113      0.078677         0.028788        0.005015         0.1   \n",
       "6       0.607519      0.062348         0.037345        0.009121           1   \n",
       "7       0.637259      0.074892         0.029107        0.009821           5   \n",
       "8       0.613111      0.072002         0.031069        0.003444          10   \n",
       "\n",
       "              params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0   {'alpha': 1e-06}           0.982051           0.970513           0.983333   \n",
       "1   {'alpha': 1e-05}           0.983333           0.971795           0.983333   \n",
       "2  {'alpha': 0.0001}           0.983333           0.973077           0.980769   \n",
       "3   {'alpha': 0.001}           0.984615           0.974359           0.983333   \n",
       "4    {'alpha': 0.01}           0.984615           0.974359           0.985897   \n",
       "5     {'alpha': 0.1}           0.982051           0.982051           0.982051   \n",
       "6       {'alpha': 1}           0.980769           0.980769           0.982051   \n",
       "7       {'alpha': 5}           0.962821           0.967949           0.967949   \n",
       "8      {'alpha': 10}           0.947436           0.939744           0.951282   \n",
       "\n",
       "   split3_test_score       ...         mean_test_score  std_test_score  \\\n",
       "0           0.974359       ...                0.977692        0.004770   \n",
       "1           0.975641       ...                0.978462        0.004471   \n",
       "2           0.974359       ...                0.977949        0.003838   \n",
       "3           0.974359       ...                0.978974        0.004336   \n",
       "4           0.975641       ...                0.980513        0.004686   \n",
       "5           0.980769       ...                0.980513        0.002486   \n",
       "6           0.984615       ...                0.980000        0.004336   \n",
       "7           0.961538       ...                0.964359        0.002968   \n",
       "8           0.943590       ...                0.946667        0.004485   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                7            0.997115            0.997115   \n",
       "1                5            0.997115            0.997115   \n",
       "2                6            0.997115            0.997115   \n",
       "3                4            0.996795            0.996795   \n",
       "4                1            0.996154            0.996795   \n",
       "5                1            0.994551            0.995833   \n",
       "6                3            0.993269            0.993910   \n",
       "7                8            0.973397            0.972436   \n",
       "8                9            0.952244            0.955449   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.996474            0.996795            0.997436   \n",
       "1            0.996474            0.996795            0.997436   \n",
       "2            0.996474            0.996795            0.997115   \n",
       "3            0.995833            0.996474            0.997115   \n",
       "4            0.995513            0.995833            0.996795   \n",
       "5            0.995513            0.995192            0.995513   \n",
       "6            0.992308            0.992949            0.994231   \n",
       "7            0.971795            0.974679            0.975000   \n",
       "8            0.950962            0.953846            0.953205   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.996987         0.000327  \n",
       "1          0.996987         0.000327  \n",
       "2          0.996923         0.000256  \n",
       "3          0.996603         0.000435  \n",
       "4          0.996218         0.000513  \n",
       "5          0.995321         0.000435  \n",
       "6          0.993333         0.000684  \n",
       "7          0.973462         0.001240  \n",
       "8          0.953141         0.001509  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a full grid over several parameters and cross validate 5 times\n",
    "pd.set_option('display.max_rows', 15) #sets output to desired length\n",
    "param_grid = {\"alpha\": [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,5,10]}\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_nb1, param_grid=param_grid,n_jobs=-1,cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_search.best_params_) \n",
    "print(\"Best Score:\", grid_search.best_score_) \n",
    "print(\"Best Estimator\", grid_search.best_estimator_) \n",
    "print(\"Grid Scores:\")\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search determined an alpha of 0.01 was optimal.  Let's try it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "0.986244019138756\n",
      "accuracy: 0.986244019138756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      1450\n",
      "       spam       0.93      0.96      0.95       222\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1435   15]\n",
      " [   8  214]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit a Naive Bayes model_nb2 to the data\n",
    "model_nb2 = MultinomialNB(alpha=0.01)\n",
    "print(model_nb2)\n",
    "model_nb2.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf6_expected = y_test\n",
    "clf6_predicted = model_nb2.predict(X_test)\n",
    "\n",
    "print(model_nb2.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_nb2\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf6_expected, clf6_predicted)))\n",
    "print(metrics.classification_report(clf6_expected, clf6_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf6_expected, clf6_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance measures improved, with false negatives dropping from 14 to 8 and false positives from 18 to 15.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try tweaking the Logistic Regression.  We'll start with a grid search on C, the inverse of regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 40}\n",
      "Best Score: 0.978974358974359\n",
      "Best Estimator LogisticRegression(C=40, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=2112, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Grid Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.669796</td>\n",
       "      <td>0.046188</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.975641</td>\n",
       "      <td>0.978205</td>\n",
       "      <td>0.979487</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976154</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>12</td>\n",
       "      <td>0.998397</td>\n",
       "      <td>0.998718</td>\n",
       "      <td>0.997756</td>\n",
       "      <td>0.997756</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612292</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.021282</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 5}</td>\n",
       "      <td>0.975641</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978462</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.999231</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.604005</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978462</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.613400</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.040950</td>\n",
       "      <td>0.038479</td>\n",
       "      <td>20</td>\n",
       "      <td>{'C': 20}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978462</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.632172</td>\n",
       "      <td>0.068697</td>\n",
       "      <td>0.021692</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>30</td>\n",
       "      <td>{'C': 30}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978718</td>\n",
       "      <td>0.004770</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.621475</td>\n",
       "      <td>0.033528</td>\n",
       "      <td>0.022145</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>40</td>\n",
       "      <td>{'C': 40}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978974</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.034318</td>\n",
       "      <td>0.021401</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>50</td>\n",
       "      <td>{'C': 50}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978974</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.626175</td>\n",
       "      <td>0.038764</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>60</td>\n",
       "      <td>{'C': 60}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978718</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.071410</td>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>70</td>\n",
       "      <td>{'C': 70}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978718</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.613346</td>\n",
       "      <td>0.030323</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>80</td>\n",
       "      <td>{'C': 80}</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978718</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.658772</td>\n",
       "      <td>0.062151</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>90</td>\n",
       "      <td>{'C': 90}</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978974</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.627079</td>\n",
       "      <td>0.067047</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978974</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.669796      0.046188         0.043245        0.010555       1   \n",
       "1        0.612292      0.023710         0.021282        0.000311       5   \n",
       "2        0.604005      0.035036         0.021785        0.000541      10   \n",
       "3        0.613400      0.013547         0.040950        0.038479      20   \n",
       "4        0.632172      0.068697         0.021692        0.000303      30   \n",
       "5        0.621475      0.033528         0.022145        0.000352      40   \n",
       "6        0.626800      0.034318         0.021401        0.000619      50   \n",
       "7        0.626175      0.038764         0.021832        0.000406      60   \n",
       "8        0.628527      0.071410         0.021326        0.000400      70   \n",
       "9        0.613346      0.030323         0.021514        0.000686      80   \n",
       "10       0.658772      0.062151         0.021623        0.000512      90   \n",
       "11       0.627079      0.067047         0.029808        0.011511     100   \n",
       "\n",
       "        params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0     {'C': 1}           0.975641           0.978205           0.979487   \n",
       "1     {'C': 5}           0.975641           0.982051           0.982051   \n",
       "2    {'C': 10}           0.974359           0.984615           0.982051   \n",
       "3    {'C': 20}           0.973077           0.985897           0.982051   \n",
       "4    {'C': 30}           0.973077           0.985897           0.982051   \n",
       "5    {'C': 40}           0.973077           0.985897           0.983333   \n",
       "6    {'C': 50}           0.973077           0.985897           0.983333   \n",
       "7    {'C': 60}           0.973077           0.984615           0.983333   \n",
       "8    {'C': 70}           0.973077           0.984615           0.983333   \n",
       "9    {'C': 80}           0.973077           0.984615           0.983333   \n",
       "10   {'C': 90}           0.974359           0.984615           0.983333   \n",
       "11  {'C': 100}           0.974359           0.984615           0.983333   \n",
       "\n",
       "    split3_test_score       ...         mean_test_score  std_test_score  \\\n",
       "0            0.967949       ...                0.976154        0.004336   \n",
       "1            0.973077       ...                0.978462        0.003571   \n",
       "2            0.973077       ...                0.978462        0.004397   \n",
       "3            0.973077       ...                0.978462        0.005025   \n",
       "4            0.974359       ...                0.978718        0.004770   \n",
       "5            0.974359       ...                0.978974        0.004972   \n",
       "6            0.974359       ...                0.978974        0.004972   \n",
       "7            0.974359       ...                0.978718        0.004630   \n",
       "8            0.974359       ...                0.978718        0.004630   \n",
       "9            0.974359       ...                0.978718        0.004630   \n",
       "10           0.974359       ...                0.978974        0.004336   \n",
       "11           0.974359       ...                0.978974        0.004336   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                12            0.998397            0.998718   \n",
       "1                 9            1.000000            0.999038   \n",
       "2                 9            1.000000            1.000000   \n",
       "3                 9            1.000000            1.000000   \n",
       "4                 5            1.000000            1.000000   \n",
       "5                 1            1.000000            1.000000   \n",
       "6                 1            1.000000            1.000000   \n",
       "7                 5            1.000000            1.000000   \n",
       "8                 5            1.000000            1.000000   \n",
       "9                 5            1.000000            1.000000   \n",
       "10                1            1.000000            1.000000   \n",
       "11                1            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.997756            0.997756            0.997436   \n",
       "1             0.999038            0.999038            0.999038   \n",
       "2             1.000000            1.000000            1.000000   \n",
       "3             1.000000            1.000000            1.000000   \n",
       "4             1.000000            1.000000            1.000000   \n",
       "5             1.000000            1.000000            1.000000   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "7             1.000000            1.000000            1.000000   \n",
       "8             1.000000            1.000000            1.000000   \n",
       "9             1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.998013         0.000471  \n",
       "1           0.999231         0.000385  \n",
       "2           1.000000         0.000000  \n",
       "3           1.000000         0.000000  \n",
       "4           1.000000         0.000000  \n",
       "5           1.000000         0.000000  \n",
       "6           1.000000         0.000000  \n",
       "7           1.000000         0.000000  \n",
       "8           1.000000         0.000000  \n",
       "9           1.000000         0.000000  \n",
       "10          1.000000         0.000000  \n",
       "11          1.000000         0.000000  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a full grid over several parameters and cross validate 5 times\n",
    "\n",
    "pd.set_option('display.max_rows', 15) #sets output to desired length\n",
    "param_grid = {\"C\": [1,5,10,20,30,40,50,60,70,80,90,100]}\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model_lr1, param_grid=param_grid,n_jobs=-1,cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_search.best_params_) \n",
    "print(\"Best Score:\", grid_search.best_score_) \n",
    "print(\"Best Estimator\", grid_search.best_estimator_) \n",
    "print(\"Grid Scores:\")\n",
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search determined that C=40 is the optimum value.  Let's put that in the model and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=40, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=2112, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.986244019138756\n",
      "accuracy: 0.986244019138756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99      1450\n",
      "       spam       0.99      0.91      0.95       222\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1448    2]\n",
      " [  21  201]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a logistic regression model to the data\n",
    "model_lr2 = LogisticRegression(C=40, random_state = 2112)\n",
    "print(model_lr2)\n",
    "model_lr2.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf4_expected = y_test\n",
    "clf4_predicted = model_lr2.predict(X_test)\n",
    "\n",
    "print(model_lr2.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf4_expected, clf4_predicted)))\n",
    "print(metrics.classification_report(clf4_expected, clf4_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf4_expected, clf4_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweak improved accuracy slightly from 0.982 to 0.986, with false negatives dropping from 26 to 21 and false positives dropping from 4 to 2.  Now let's try changing the penalty from L2 to L1 (Ridge to LASSO, thus allowing less important feature coefficients to shrink to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=40, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=2112, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9832535885167464\n",
      "accuracy: 0.9832535885167464\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99      1450\n",
      "       spam       0.98      0.90      0.93       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1445    5]\n",
      " [  23  199]]\n"
     ]
    }
   ],
   "source": [
    "# fit a logistic regression model to the data\n",
    "model_lr3 = LogisticRegression(C=40, penalty='l1',random_state = 2112)\n",
    "print(model_lr3)\n",
    "model_lr3.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf5_expected = y_test\n",
    "clf5_predicted = model_lr3.predict(X_test)\n",
    "\n",
    "print(model_lr3.score(X_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf5_expected, clf5_predicted)))\n",
    "print(metrics.classification_report(clf5_expected, clf5_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf5_expected, clf5_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that switching from L2 to L1 resulted in slightly lower measurements, with false negatives increasing from 21 to 23, and false positives from 2 to 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model tuning with stemmed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with further tweaks, we'll try preprocessing and run the models again.  In this business case with a wide variety of alternative spellings and abbreviations, it makes sense to try stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Stemming the words in the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll stem the words and get a new word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines our stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0      0    0       0             0     0     0            0            0   \n",
       "1      0    0       0             0     0     0            0            0   \n",
       "2      0    0       0             0     0     0            0            0   \n",
       "3      0    0       0             0     0     0            0            0   \n",
       "4      0    0       0             0     0     0            0            0   \n",
       "...   ..  ...     ...           ...   ...   ...          ...          ...   \n",
       "5567   0    0       0             0     0     0            0            0   \n",
       "5568   0    0       0             0     0     0            0            0   \n",
       "5569   0    0       0             0     0     0            0            0   \n",
       "5570   0    0       0             0     0     0            0            0   \n",
       "5571   0    0       0             0     0     0            0            0   \n",
       "\n",
       "      0125698789  02 ...   zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \\\n",
       "0              0   0 ...       0       0    0          0     0     0      0   \n",
       "1              0   0 ...       0       0    0          0     0     0      0   \n",
       "2              0   0 ...       0       0    0          0     0     0      0   \n",
       "3              0   0 ...       0       0    0          0     0     0      0   \n",
       "4              0   0 ...       0       0    0          0     0     0      0   \n",
       "...          ...  .. ...     ...     ...  ...        ...   ...   ...    ...   \n",
       "5567           0   0 ...       0       0    0          0     0     0      0   \n",
       "5568           0   0 ...       0       0    0          0     0     0      0   \n",
       "5569           0   0 ...       0       0    0          0     0     0      0   \n",
       "5570           0   0 ...       0       0    0          0     0     0      0   \n",
       "5571           0   0 ...       0       0    0          0     0     0      0   \n",
       "\n",
       "      èn  ú1  〨ud  \n",
       "0      0   0    0  \n",
       "1      0   0    0  \n",
       "2      0   0    0  \n",
       "3      0   0    0  \n",
       "4      0   0    0  \n",
       "...   ..  ..  ...  \n",
       "5567   0   0    0  \n",
       "5568   0   0    0  \n",
       "5569   0   0    0  \n",
       "5570   0   0    0  \n",
       "5571   0   0    0  \n",
       "\n",
       "[5572 rows x 8256 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textStr_stem = spam_data['Message'] #creates a generic name for the headline text; useful for reusing the code for other projects \n",
    "\n",
    "textStr_stem = textStr_stem.apply(stem_sentences) #applies the stemmer defined above\n",
    "\n",
    "cv2 = CountVectorizer(binary=False, lowercase=True) \n",
    "#thought that mutiple uses of the same words might give the model more predictive power, thus kept binary to False\n",
    "\n",
    "cv2_chat = cv2.fit_transform(textStr_stem) #transforms text\n",
    "\n",
    "word_count_stem = pd.DataFrame(cv2_chat.toarray(),columns = cv2.get_feature_names())\n",
    "\n",
    "pd.set_option('display.max_rows', 10) #sets output to desired length\n",
    "word_count_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the Porter stemmer reduced the feature space from 8,709 to 8,256.  Now let's conduct the test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 8256)\n",
      "(1672, 8256)\n",
      "(3900,)\n",
      "(1672,)\n"
     ]
    }
   ],
   "source": [
    "y = spam_data['Category'].values #this is an array of labels\n",
    "X_stem = word_count_stem #changed name since feature space is different after stemming\n",
    "\n",
    "X_stem_train, X_stem_test, y_train, y_test = train_test_split(X_stem, y, test_size=0.3, random_state=2112) #random_state is set seed\n",
    "\n",
    "print(X_stem_train.shape)\n",
    "print(X_stem_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the test/train split is complete, let's run the three different models using the previously identified tweaks that produced the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll start with the decision tree.  We'll use the tweaks from the second decision tree (using 11 max features) even though it had lower accuracy, since it had better precision (and thus less ham being identified as spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=11, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=2112,\n",
      "            splitter='best')\n",
      "0.9360047846889952\n",
      "accuracy: 0.9360047846889952\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.95      0.98      0.96      1450\n",
      "       spam       0.81      0.68      0.74       222\n",
      "\n",
      "avg / total       0.93      0.94      0.93      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1415   35]\n",
      " [  72  150]]\n"
     ]
    }
   ],
   "source": [
    "# fit a Decision Tree model_dt3 to the data\n",
    "model_dt3 = DecisionTreeClassifier(max_features = 11, random_state = 2112)\n",
    "print(model_dt3)\n",
    "model_dt3.fit(X_stem_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf10_expected = y_test\n",
    "clf10_predicted = model_dt3.predict(X_stem_test)\n",
    "\n",
    "print(model_dt3.score(X_stem_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_dt3\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf10_expected, clf10_predicted)))\n",
    "print(metrics.classification_report(clf10_expected, clf10_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf10_expected, clf10_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy decreased from 0.962 to 0.936 using stemmed words.  Now let's try the Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an alpha of 0.01, since that produced the best result previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "0.9838516746411483\n",
      "accuracy: 0.9838516746411483\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      1450\n",
      "       spam       0.92      0.96      0.94       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1432   18]\n",
      " [   9  213]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit a Naive Bayes model_nb3 to the data\n",
    "model_nb3 = MultinomialNB(alpha=0.01)\n",
    "print(model_nb3)\n",
    "model_nb3.fit(X_stem_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf9_expected = y_test\n",
    "clf9_predicted = model_nb3.predict(X_stem_test)\n",
    "\n",
    "print(model_nb3.score(X_stem_test, y_test))\n",
    "\n",
    "# summarize the fit of the model_nb3\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf9_expected, clf9_predicted)))\n",
    "print(metrics.classification_report(clf9_expected, clf9_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf9_expected, clf9_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy decreased slighly from 0.986 to 0.984 using the stemmed words with the Naive Bayes.  Now let's try the logistic regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previsouly identified C=40 as the best, so we'll use that again here with the stemmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=40, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=2112, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9844497607655502\n",
      "accuracy: 0.9844497607655502\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99      1450\n",
      "       spam       0.98      0.90      0.94       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1672\n",
      "\n",
      "Confusion Matrix\n",
      "[[1446    4]\n",
      " [  22  200]]\n"
     ]
    }
   ],
   "source": [
    "# fit a logistic regression model to the data\n",
    "model_lr4 = LogisticRegression(C=40, random_state = 2112)\n",
    "print(model_lr4)\n",
    "model_lr4.fit(X_stem_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "clf8_expected = y_test\n",
    "clf8_predicted = model_lr4.predict(X_stem_test)\n",
    "\n",
    "print(model_lr4.score(X_stem_test, y_test))\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(clf8_expected, clf8_predicted)))\n",
    "print(metrics.classification_report(clf8_expected, clf8_predicted))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(clf8_expected, clf8_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy decreased slightly from 0.986 to 0.984 using the stemmed feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Comparing All Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll list the performance measures side by side for easier comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree (Default)</th>\n",
       "      <td>1436</td>\n",
       "      <td>187</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>0.970694</td>\n",
       "      <td>0.930348</td>\n",
       "      <td>0.842342</td>\n",
       "      <td>0.884161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree (11 Max Features)</th>\n",
       "      <td>1440</td>\n",
       "      <td>169</td>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962321</td>\n",
       "      <td>0.944134</td>\n",
       "      <td>0.761261</td>\n",
       "      <td>0.842893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree (11 Max Features, Stemmed)</th>\n",
       "      <td>1415</td>\n",
       "      <td>150</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0.936005</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.737101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes (Default)</th>\n",
       "      <td>1432</td>\n",
       "      <td>208</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0.980861</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.936937</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes (α=0.01)</th>\n",
       "      <td>1435</td>\n",
       "      <td>214</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.986244</td>\n",
       "      <td>0.934498</td>\n",
       "      <td>0.963964</td>\n",
       "      <td>0.949002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes (α=0.01, Stemmed)</th>\n",
       "      <td>1432</td>\n",
       "      <td>213</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.983852</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.940397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (Default)</th>\n",
       "      <td>1446</td>\n",
       "      <td>196</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982057</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.928910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (C=40)</th>\n",
       "      <td>1448</td>\n",
       "      <td>201</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.986244</td>\n",
       "      <td>0.990148</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.945882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (C=40, L1)</th>\n",
       "      <td>1445</td>\n",
       "      <td>199</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>0.975490</td>\n",
       "      <td>0.896396</td>\n",
       "      <td>0.934272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (C=40, Stemmed)</th>\n",
       "      <td>1446</td>\n",
       "      <td>200</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0.984450</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.900901</td>\n",
       "      <td>0.938967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TN   TP  FN  FP  Accuracy  \\\n",
       "Model                                                                   \n",
       "Decision Tree (Default)                   1436  187  35  14  0.970694   \n",
       "Decision Tree (11 Max Features)           1440  169  53  10  0.962321   \n",
       "Decision Tree (11 Max Features, Stemmed)  1415  150  72  35  0.936005   \n",
       "Naive Bayes (Default)                     1432  208  14  18  0.980861   \n",
       "Naive Bayes (α=0.01)                      1435  214   8  15  0.986244   \n",
       "Naive Bayes (α=0.01, Stemmed)             1432  213   9  18  0.983852   \n",
       "Logistic Regression (Default)             1446  196  26   4  0.982057   \n",
       "Logistic Regression (C=40)                1448  201  21   2  0.986244   \n",
       "Logistic Regression (C=40, L1)            1445  199  23   5  0.983254   \n",
       "Logistic Regression (C=40, Stemmed)       1446  200  22   4  0.984450   \n",
       "\n",
       "                                          Precision    Recall  F1 Score  \n",
       "Model                                                                    \n",
       "Decision Tree (Default)                    0.930348  0.842342  0.884161  \n",
       "Decision Tree (11 Max Features)            0.944134  0.761261  0.842893  \n",
       "Decision Tree (11 Max Features, Stemmed)   0.810811  0.675676  0.737101  \n",
       "Naive Bayes (Default)                      0.920354  0.936937  0.928571  \n",
       "Naive Bayes (α=0.01)                       0.934498  0.963964  0.949002  \n",
       "Naive Bayes (α=0.01, Stemmed)              0.922078  0.959459  0.940397  \n",
       "Logistic Regression (Default)              0.980000  0.882883  0.928910  \n",
       "Logistic Regression (C=40)                 0.990148  0.905405  0.945882  \n",
       "Logistic Regression (C=40, L1)             0.975490  0.896396  0.934272  \n",
       "Logistic Regression (C=40, Stemmed)        0.980392  0.900901  0.938967  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = {'Model': ['Decision Tree (Default)',\n",
    "                          'Decision Tree (11 Max Features)',\n",
    "                          'Decision Tree (11 Max Features, Stemmed)',\n",
    "                          'Naive Bayes (Default)',\n",
    "                          'Naive Bayes (α=0.01)',\n",
    "                          'Naive Bayes (α=0.01, Stemmed)',\n",
    "                          'Logistic Regression (Default)',\n",
    "                          'Logistic Regression (C=40)',\n",
    "                          'Logistic Regression (C=40, L1)',\n",
    "                          'Logistic Regression (C=40, Stemmed)'],\n",
    "                'TN': [1436, 1440, 1415, 1432, 1435, 1432, 1446, 1448, 1445, 1446],\n",
    "                'TP': [187, 169, 150, 208, 214, 213, 196, 201, 199, 200],\n",
    "                'FN' : [35, 53, 72, 14, 8, 9, 26, 21, 23, 22],\n",
    "                'FP' : [14, 10, 35, 18, 15, 18, 4, 2, 5, 4]}\n",
    "              \n",
    "model_metrics = pd.DataFrame(data=metrics_data) #creates a dataframe\n",
    "model_metrics = model_metrics.set_index('Model') #sets model as the row index\n",
    "total = model_metrics['TN']+model_metrics['TP']+model_metrics['FN']+model_metrics['FP']\n",
    "model_metrics['Accuracy'] = (model_metrics['TN'] + model_metrics['TP'])/total #calculates accuracy\n",
    "model_metrics['Precision'] = model_metrics['TP']/(model_metrics['TP']+model_metrics['FP']) #calculates precision\n",
    "model_metrics['Recall'] = model_metrics['TP']/(model_metrics['TP']+model_metrics['FN']) #calculates recall\n",
    "model_metrics['F1 Score'] = 2*model_metrics['Precision']*model_metrics['Recall']/(model_metrics['Precision']+model_metrics['Recall']) #calculates F1 score\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see all performance measures in the table above.  Two models are tied for highest accuracy, the Naive Bayes with an alpha of 0.01, and the logisitic regression with C=40, in both cases without using stemming.  Of these two, the Naive Bayes had highest recall across all models, while the logistic regression had the highest precision across all models.  As mentioned previously, I judge precision as more important than recall here since I'd rather get a little more spam than miss ham; thus, I rate the logisitic regression (C=40, not stemmed, L2) as the strongest model for this business question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 is to \"Perform K-means clustering on your dataset.  Store the cluster assignments in the dataframe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't know what the optimal number of clusters is, we'll run a range of K values to determine the right numbers.  First running the K-means on a range of 1 to 20:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many clusters? this takes some time\n",
    "\n",
    "# finding an optimal value for k\n",
    "k_range = range(1,20)\n",
    "k_means_set = [KMeans(n_clusters=k,init='k-means++', max_iter=100, random_state = 2112).fit(word_count) for k in k_range]\n",
    "centroids_list = [km_result.cluster_centers_ for km_result in k_means_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the distance from each point to the cluster's center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc euclidean dist from each point to each cluster center\n",
    "\n",
    "k_euclid = [cdist(word_count, thing, 'euclidean') for thing in centroids_list]\n",
    "distance_set = [np.min(k_euc, axis=1) for k_euc in k_euclid]\n",
    "\n",
    "# total within-cluster sum of squares\n",
    "wcss = [np.sum(distance**2) for distance in distance_set]\n",
    "\n",
    "# total sum of squares\n",
    "tss  = np.sum(pdist(word_count)**2) / word_count.shape[0]\n",
    "\n",
    "# between cluster sum of squares\n",
    "bss = tss - wcss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'% Var Explained vs K')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot elbow chart\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(k_range, bss/tss*100, '^-')\n",
    "ax.set_ylim((0,15))\n",
    "plt.grid(True)\n",
    "plt.xlabel('K n_clusters')\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.title('% Var Explained vs K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis shows that the percent of variance explained steadily creeps up slowly, but there's a 'knee' in the curve at 7 clusters; there's little improvement for each additional cluster added beyond that point.  We'll run the analysis on 7 clusters, though arguments could be made to use a different number.  The low percentage of variance explained by clustering suggests that there's little information gained from K-means for this dataset, but let's look at 7 clusters to see if there's any information to be gleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##K-means with 7 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=2112, tol=0.0001, verbose=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster\n",
       "0           6\n",
       "1           6\n",
       "2           2\n",
       "3           6\n",
       "4           6\n",
       "...       ...\n",
       "5567        5\n",
       "5568        6\n",
       "5569        6\n",
       "5570        5\n",
       "5571        6\n",
       "\n",
       "[5572 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10) #sets output to desired length\n",
    "\n",
    "model_km1 = KMeans(n_clusters=7, init='k-means++', max_iter=100, random_state = 2112) #sets the cluster parameters\n",
    "print(model_km1)\n",
    "\n",
    "model_km1.fit(word_count)  \n",
    "labels = model_km1.labels_\n",
    "\n",
    "km1_results = pd.DataFrame(data=labels, columns=['Cluster'])\n",
    "\n",
    "km1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cluster assignment for each text message in the dataset.  Let's combine this and the original dataset to see if there's any correlation between cluster and spam/ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "0         ham   \n",
       "1         ham   \n",
       "2        spam   \n",
       "3         ham   \n",
       "4         ham   \n",
       "...       ...   \n",
       "5567     spam   \n",
       "5568      ham   \n",
       "5569      ham   \n",
       "5570      ham   \n",
       "5571      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "0                                           Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                             Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007...   \n",
       "3                                                                                                         U dun say so early hor... U c already then say...   \n",
       "4                                                                                             Nah I don't think he goes to usf, he lives around here though   \n",
       "...                                                                                                                                                     ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...   \n",
       "5568                                                                                                                   Will ü b going to esplanade fr home?   \n",
       "5569                                                                                              Pity, * was in mood for that. So...any other suggestions?   \n",
       "5570                          The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free   \n",
       "5571                                                                                                                             Rofl. Its true to its name   \n",
       "\n",
       "      Cluster  \n",
       "0           6  \n",
       "1           6  \n",
       "2           2  \n",
       "3           6  \n",
       "4           6  \n",
       "...       ...  \n",
       "5567        5  \n",
       "5568        6  \n",
       "5569        6  \n",
       "5570        5  \n",
       "5571        6  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10) #sets output to desired length\n",
    "cluster_results = spam_data\n",
    "cluster_results['Cluster'] = km1_results\n",
    "cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combining was successful, but it's not yet clear how cluster correlates with the texts. First let's see the correlation between cluster and category (ham or spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">ham</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">spam</th>\n",
       "      <th>1</th>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Message\n",
       "Category Cluster         \n",
       "ham      0              1\n",
       "         1           1014\n",
       "         2            188\n",
       "         3            186\n",
       "         4              1\n",
       "         5            514\n",
       "         6           2921\n",
       "spam     1            138\n",
       "         2            373\n",
       "         3              7\n",
       "         5             48\n",
       "         6            181"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 14) #sets output to desired length\n",
    "cluster_results.groupby(['Category','Cluster']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard at first glance to see how this adds information.  Cluster 2 is spam rich (66.5%), but clusters 0 and 4 have only one message each, neither being spam, making them effectively meaningless.  In order to get a sense of what texts are assigned to each cluster, let's look at some examples of each (except for 0 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>Is that seriously how you spell his name?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with you sooner?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm back &amp;amp; we're packing the car now, I'll let you know if there's room</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ham</td>\n",
       "      <td>K tell me anything about you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spam</td>\n",
       "      <td>Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oops, I'll let you know when my roommate's done</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ham</td>\n",
       "      <td>WHO ARE YOU SEEING?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ham</td>\n",
       "      <td>Great! I hope you like your man well endowed. I am  &amp;lt;#&amp;gt;  inches...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ham</td>\n",
       "      <td>Didn't you get hep b immunisation in nigeria.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>ham</td>\n",
       "      <td>I want to tell you how bad I feel that basically the only times I text you lately are when I need drugs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>ham</td>\n",
       "      <td>Total disappointment, when I texted you was the craziest shit got :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>ham</td>\n",
       "      <td>Compliments to you. Was away from the system. How your side.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>ham</td>\n",
       "      <td>happened here while you were adventuring</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi its in durban are you still on this number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>ham</td>\n",
       "      <td>You know, wot people wear. T shirts, jumpers, hat, belt, is all we know. We r at Cribbs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, what time you think you can get here?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wen did you get so spiritual and deep. That's great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes i have. So that's why u texted. Pshew...missing you so much</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to see if you get your .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "20        ham   \n",
       "24        ham   \n",
       "28        ham   \n",
       "30        ham   \n",
       "32        ham   \n",
       "34       spam   \n",
       "36        ham   \n",
       "43        ham   \n",
       "44        ham   \n",
       "46        ham   \n",
       "...       ...   \n",
       "5525      ham   \n",
       "5527      ham   \n",
       "5531      ham   \n",
       "5532      ham   \n",
       "5545      ham   \n",
       "5549      ham   \n",
       "5550      ham   \n",
       "5551      ham   \n",
       "5556      ham   \n",
       "5564      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "20                                                                                                                Is that seriously how you spell his name?   \n",
       "24                                                                                                Ffffffffff. Alright no way I can meet up with you sooner?   \n",
       "28                                                                              I'm back &amp; we're packing the car now, I'll let you know if there's room   \n",
       "30                       Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us   \n",
       "32                                                                                                                            K tell me anything about you.   \n",
       "34    Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will no...   \n",
       "36                                                                                                          Oops, I'll let you know when my roommate's done   \n",
       "43                                                                                                                                      WHO ARE YOU SEEING?   \n",
       "44                                                                                 Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...   \n",
       "46                                                                                                            Didn't you get hep b immunisation in nigeria.   \n",
       "...                                                                                                                                                     ...   \n",
       "5525                                                I want to tell you how bad I feel that basically the only times I text you lately are when I need drugs   \n",
       "5527                                                                                   Total disappointment, when I texted you was the craziest shit got :(   \n",
       "5531                                                                                           Compliments to you. Was away from the system. How your side.   \n",
       "5532                                                                                                               happened here while you were adventuring   \n",
       "5545                                                                                                          Hi its in durban are you still on this number   \n",
       "5549                                                                You know, wot people wear. T shirts, jumpers, hat, belt, is all we know. We r at Cribbs   \n",
       "5550                                                                                                            Cool, what time you think you can get here?   \n",
       "5551                                                                                                    Wen did you get so spiritual and deep. That's great   \n",
       "5556                                                                                        Yes i have. So that's why u texted. Pshew...missing you so much   \n",
       "5564                                                                                    Why don't you wait 'til at least wednesday to see if you get your .   \n",
       "\n",
       "      Cluster  \n",
       "20          1  \n",
       "24          1  \n",
       "28          1  \n",
       "30          1  \n",
       "32          1  \n",
       "34          1  \n",
       "36          1  \n",
       "43          1  \n",
       "44          1  \n",
       "46          1  \n",
       "...       ...  \n",
       "5525        1  \n",
       "5527        1  \n",
       "5531        1  \n",
       "5532        1  \n",
       "5545        1  \n",
       "5549        1  \n",
       "5550        1  \n",
       "5551        1  \n",
       "5556        1  \n",
       "5564        1  \n",
       "\n",
       "[1152 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 20) #sets output to desired length\n",
    "cluster_results[cluster_results.Cluster==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 is only 12% spam, and these messages look like normal texts, and the spam example here is using fairly normal English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your frie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 080...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 4403LD...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>spam</td>\n",
       "      <td>07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 0800093...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ham</td>\n",
       "      <td>U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>spam</td>\n",
       "      <td>December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>spam</td>\n",
       "      <td>Txt: CALL to No: 86888 &amp; claim your reward of 3 hours talk time to use from your phone now! Subscribe6GBP/mnth inc 3hrs 16 stop?txtStop www.gamb.tv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>spam</td>\n",
       "      <td>Get your garden ready for summer with a FREE selection of summer bulbs and seeds worth £33:50 only with The Scotsman this Saturday. To stop go2 no...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>spam</td>\n",
       "      <td>2p per min to call Germany 08448350055 from your BT line. Just 2p per min. Check PlanetTalkInstant.com for info &amp; T's &amp; C's. Text stop to opt out</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, we shall go and see, have to go to tip anyway. Are you at home, got something to drop in later? So lets go to town tonight! Maybe mum can ta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ya, i'm referin to mei's ex wat... No ah, waitin 4 u to treat, somebody shld b rich liao...So gd, den u dun have to work frm tmr onwards...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Motorola, Nokia etc. all FREE! Double Mins &amp; Text on Orange tariffs. TEXT YES for callback, no to remove...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>ham</td>\n",
       "      <td>Get me out of this dump heap. My mom decided to come to lowes. BORING.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "2        spam   \n",
       "5        spam   \n",
       "7         ham   \n",
       "8        spam   \n",
       "9        spam   \n",
       "11       spam   \n",
       "12       spam   \n",
       "19       spam   \n",
       "42       spam   \n",
       "49        ham   \n",
       "...       ...   \n",
       "5460     spam   \n",
       "5462     spam   \n",
       "5467     spam   \n",
       "5487     spam   \n",
       "5494      ham   \n",
       "5516      ham   \n",
       "5547     spam   \n",
       "5552      ham   \n",
       "5561      ham   \n",
       "5566     spam   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007...   \n",
       "5       FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv   \n",
       "7     As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your frie...   \n",
       "8     WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 ...   \n",
       "9     Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 080...   \n",
       "11                 SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info   \n",
       "12    URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LD...   \n",
       "19    England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504...   \n",
       "42    07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 0800093...   \n",
       "49     U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers.   \n",
       "...                                                                                                                                                     ...   \n",
       "5460  December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on ...   \n",
       "5462    Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GBP/mnth inc 3hrs 16 stop?txtStop www.gamb.tv   \n",
       "5467  Get your garden ready for summer with a FREE selection of summer bulbs and seeds worth £33:50 only with The Scotsman this Saturday. To stop go2 no...   \n",
       "5487      2p per min to call Germany 08448350055 from your BT line. Just 2p per min. Check PlanetTalkInstant.com for info & T's & C's. Text stop to opt out   \n",
       "5494  Cool, we shall go and see, have to go to tip anyway. Are you at home, got something to drop in later? So lets go to town tonight! Maybe mum can ta...   \n",
       "5516            Ya, i'm referin to mei's ex wat... No ah, waitin 4 u to treat, somebody shld b rich liao...So gd, den u dun have to work frm tmr onwards...   \n",
       "5547  Had your contract mobile 11 Mnths? Latest Motorola, Nokia etc. all FREE! Double Mins & Text on Orange tariffs. TEXT YES for callback, no to remove...   \n",
       "5552                                                            Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with   \n",
       "5561                                                                                 Get me out of this dump heap. My mom decided to come to lowes. BORING.   \n",
       "5566    REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode   \n",
       "\n",
       "      Cluster  \n",
       "2           2  \n",
       "5           2  \n",
       "7           2  \n",
       "8           2  \n",
       "9           2  \n",
       "11          2  \n",
       "12          2  \n",
       "19          2  \n",
       "42          2  \n",
       "49          2  \n",
       "...       ...  \n",
       "5460        2  \n",
       "5462        2  \n",
       "5467        2  \n",
       "5487        2  \n",
       "5494        2  \n",
       "5516        2  \n",
       "5547        2  \n",
       "5552        2  \n",
       "5561        2  \n",
       "5566        2  \n",
       "\n",
       "[561 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_results[cluster_results.Cluster==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 2 is 66.5% spam.  The ham messages seem longer, less well written, or perhaps have a name ('Nigeria' or 'Lowes', to give two examples) that might be more typically associated with spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. Yo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just so that you know,yetunde hasn't sent money yet. I just sent her a text not to bother sending. So its over, you dont have to involve yourself ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are everywhere dirt, on the floor, the windows, even on my shirt. And sometimes when i open my mouth, you are all that comes flowing out. I dr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello, my love. What are you doing? Did you get to that interview today? Are you you happy? Are you being a good boy? Do you think of me?Are you m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ham</td>\n",
       "      <td>Keep yourself safe for me because I need you and I miss you already and I envy everyone that see's you in real life</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm sorry. I've joined the league of people that dont keep in touch. You mean a great deal to me. You have been a friend at all times even at grea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>ham</td>\n",
       "      <td>HARD BUT TRUE: How much you show &amp;amp;  express your love to someone....that much it will hurt when they leave you or you get seperated...!鈥┾??〨ud...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>ham</td>\n",
       "      <td>So how are you really. What are you up to. How's the masters. And so on.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you can make it any time tonight or whenever you can it's cool, just text me whenever you're around</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>ham</td>\n",
       "      <td>What i told before i tell. Stupid hear after i wont tell anything to you. You dad called to my brother and spoken. Not with me.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are a great role model. You are giving so much and i really wish each day for a miracle but God as a reason for everything and i must say i wi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hey chief, can you give me a bell when you get this. Need to talk to you about this royal visit on the 1st june.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>ham</td>\n",
       "      <td>I know you are thinkin malaria. But relax, children cant handle malaria. She would have been worse and its gastroenteritis. If she takes enough to...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>ham</td>\n",
       "      <td>I can't believe how attached I am to seeing you every day. I know you will do the best you can to get to me babe. I will go to teach my class at y...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm taking derek &amp;amp; taylor to walmart, if I'm not back by the time you're done just leave the mouse on my desk and I'll text you when priscilla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. I meant the calculation is the same. That  &amp;lt;#&amp;gt; units at  &amp;lt;#&amp;gt; . This school is really expensive. Have you started practicing your a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "13        ham   \n",
       "27        ham   \n",
       "39        ham   \n",
       "41        ham   \n",
       "53        ham   \n",
       "127       ham   \n",
       "155       ham   \n",
       "158       ham   \n",
       "161       ham   \n",
       "192       ham   \n",
       "...       ...   \n",
       "5400      ham   \n",
       "5418      ham   \n",
       "5430      ham   \n",
       "5505      ham   \n",
       "5515      ham   \n",
       "5533      ham   \n",
       "5535      ham   \n",
       "5538      ham   \n",
       "5544      ham   \n",
       "5557      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "13    I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. Yo...   \n",
       "27                   Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?   \n",
       "39    Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or ...   \n",
       "41           Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy   \n",
       "53    Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always...   \n",
       "127   Just so that you know,yetunde hasn't sent money yet. I just sent her a text not to bother sending. So its over, you dont have to involve yourself ...   \n",
       "155   You are everywhere dirt, on the floor, the windows, even on my shirt. And sometimes when i open my mouth, you are all that comes flowing out. I dr...   \n",
       "158   Hello, my love. What are you doing? Did you get to that interview today? Are you you happy? Are you being a good boy? Do you think of me?Are you m...   \n",
       "161                                     Keep yourself safe for me because I need you and I miss you already and I envy everyone that see's you in real life   \n",
       "192   I'm sorry. I've joined the league of people that dont keep in touch. You mean a great deal to me. You have been a friend at all times even at grea...   \n",
       "...                                                                                                                                                     ...   \n",
       "5400  HARD BUT TRUE: How much you show &amp;  express your love to someone....that much it will hurt when they leave you or you get seperated...!鈥┾??〨ud...   \n",
       "5418                                                                               So how are you really. What are you up to. How's the masters. And so on.   \n",
       "5430                                                 If you can make it any time tonight or whenever you can it's cool, just text me whenever you're around   \n",
       "5505                        What i told before i tell. Stupid hear after i wont tell anything to you. You dad called to my brother and spoken. Not with me.   \n",
       "5515  You are a great role model. You are giving so much and i really wish each day for a miracle but God as a reason for everything and i must say i wi...   \n",
       "5533                                       Hey chief, can you give me a bell when you get this. Need to talk to you about this royal visit on the 1st june.   \n",
       "5535  I know you are thinkin malaria. But relax, children cant handle malaria. She would have been worse and its gastroenteritis. If she takes enough to...   \n",
       "5538  I can't believe how attached I am to seeing you every day. I know you will do the best you can to get to me babe. I will go to teach my class at y...   \n",
       "5544  I'm taking derek &amp; taylor to walmart, if I'm not back by the time you're done just leave the mouse on my desk and I'll text you when priscilla...   \n",
       "5557  No. I meant the calculation is the same. That  &lt;#&gt; units at  &lt;#&gt; . This school is really expensive. Have you started practicing your a...   \n",
       "\n",
       "      Cluster  \n",
       "13          3  \n",
       "27          3  \n",
       "39          3  \n",
       "41          3  \n",
       "53          3  \n",
       "127         3  \n",
       "155         3  \n",
       "158         3  \n",
       "161         3  \n",
       "192         3  \n",
       "...       ...  \n",
       "5400        3  \n",
       "5418        3  \n",
       "5430        3  \n",
       "5505        3  \n",
       "5515        3  \n",
       "5533        3  \n",
       "5535        3  \n",
       "5538        3  \n",
       "5544        3  \n",
       "5557        3  \n",
       "\n",
       "[193 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_results[cluster_results.Cluster==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 3 is only 3.6% spam.  They look like fairly normal longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way its gota b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ham</td>\n",
       "      <td>For fear of fainting with the of all that housework you just did? Quick have a cuppa</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>spam</td>\n",
       "      <td>SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>spam</td>\n",
       "      <td>Did you hear about the new \"Divorce Barbie\"? It comes with all of Ken's stuff!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ham</td>\n",
       "      <td>Its not the same here. Still looking for a job. How much do Ta's earn there.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ham</td>\n",
       "      <td>You will be in the place of that man</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry to be a pain. Is it ok if we meet another night? I spent late afternoon in casualty and that means i haven't done any of y stuff42moro and t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>ham</td>\n",
       "      <td>Where's mummy's boy ? Is he being good or bad ? Is he being positive or negative ? Why is mummy being made to wait? Hmmmm?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>ham</td>\n",
       "      <td>, ,  and  picking them up from various points | going 2 yeovil | and they will do the motor project 4 3 hours | and then u take them home. || 12 2...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>ham</td>\n",
       "      <td>Also fuck you and your family for going to rhode island or wherever the fuck and leaving me all alone the week I have a new bong &amp;gt;:(</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>spam</td>\n",
       "      <td>Marvel Mobile Play the official Ultimate Spider-man game (£4.50) on ur mobile right now. Text SPIDER to 83338 for the game &amp; we ll send u a FREE 8...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>ham</td>\n",
       "      <td>Love has one law; Make happy the person you love. In the same way friendship has one law; Never make ur friend feel alone until you are alive.... ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>ham</td>\n",
       "      <td>By the way, i've put a skip right outside the front of the house so you can see which house it is. Just pull up before it.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>ham</td>\n",
       "      <td>Its just the effect of irritation. Just ignore it</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>ham</td>\n",
       "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hours imma flip my shit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "15       spam   \n",
       "18        ham   \n",
       "31        ham   \n",
       "33        ham   \n",
       "35        ham   \n",
       "54       spam   \n",
       "68       spam   \n",
       "79        ham   \n",
       "83        ham   \n",
       "91        ham   \n",
       "...       ...   \n",
       "5474      ham   \n",
       "5484      ham   \n",
       "5485      ham   \n",
       "5492     spam   \n",
       "5500      ham   \n",
       "5518      ham   \n",
       "5528      ham   \n",
       "5559      ham   \n",
       "5567     spam   \n",
       "5570      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "15    XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL   \n",
       "18                                                                                                 Fine if thats the way u feel. Thats the way its gota b   \n",
       "31    Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't ...   \n",
       "33                                                                     For fear of fainting with the of all that housework you just did? Quick have a cuppa   \n",
       "35                               Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am   \n",
       "54                                 SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV   \n",
       "68                                                                           Did you hear about the new \"Divorce Barbie\"? It comes with all of Ken's stuff!   \n",
       "79                                                                             Its not the same here. Still looking for a job. How much do Ta's earn there.   \n",
       "83                                                                                                                     You will be in the place of that man   \n",
       "91    Sorry to be a pain. Is it ok if we meet another night? I spent late afternoon in casualty and that means i haven't done any of y stuff42moro and t...   \n",
       "...                                                                                                                                                     ...   \n",
       "5474                             Where's mummy's boy ? Is he being good or bad ? Is he being positive or negative ? Why is mummy being made to wait? Hmmmm?   \n",
       "5484  , ,  and  picking them up from various points | going 2 yeovil | and they will do the motor project 4 3 hours | and then u take them home. || 12 2...   \n",
       "5485                Also fuck you and your family for going to rhode island or wherever the fuck and leaving me all alone the week I have a new bong &gt;:(   \n",
       "5492  Marvel Mobile Play the official Ultimate Spider-man game (£4.50) on ur mobile right now. Text SPIDER to 83338 for the game & we ll send u a FREE 8...   \n",
       "5500  Love has one law; Make happy the person you love. In the same way friendship has one law; Never make ur friend feel alone until you are alive.... ...   \n",
       "5518                             By the way, i've put a skip right outside the front of the house so you can see which house it is. Just pull up before it.   \n",
       "5528                                                                                                      Its just the effect of irritation. Just ignore it   \n",
       "5559                                                                                     if you aren't here in the next  &lt;#&gt;  hours imma flip my shit   \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-...   \n",
       "5570                          The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free   \n",
       "\n",
       "      Cluster  \n",
       "15          5  \n",
       "18          5  \n",
       "31          5  \n",
       "33          5  \n",
       "35          5  \n",
       "54          5  \n",
       "68          5  \n",
       "79          5  \n",
       "83          5  \n",
       "91          5  \n",
       "...       ...  \n",
       "5474        5  \n",
       "5484        5  \n",
       "5485        5  \n",
       "5492        5  \n",
       "5500        5  \n",
       "5518        5  \n",
       "5528        5  \n",
       "5559        5  \n",
       "5567        5  \n",
       "5570        5  \n",
       "\n",
       "[562 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_results[cluster_results.Cluster==5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 5 is 8.5% spam.  I don't see any particular correlation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeh. Indians was nice. Tho it did kane me off a bit he he. We shud go out 4 a drink sometime soon. Mite hav 2 go 2 da works 4 a laugh soon. Love P...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>ham</td>\n",
       "      <td>Anything lor. Juz both of us lor.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3102 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  \\\n",
       "0         ham   \n",
       "1         ham   \n",
       "3         ham   \n",
       "4         ham   \n",
       "6         ham   \n",
       "10        ham   \n",
       "14        ham   \n",
       "16        ham   \n",
       "17        ham   \n",
       "21        ham   \n",
       "...       ...   \n",
       "5554      ham   \n",
       "5555      ham   \n",
       "5558      ham   \n",
       "5560      ham   \n",
       "5562      ham   \n",
       "5563      ham   \n",
       "5565      ham   \n",
       "5568      ham   \n",
       "5569      ham   \n",
       "5571      ham   \n",
       "\n",
       "                                                                                                                                                    Message  \\\n",
       "0                                           Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                             Ok lar... Joking wif u oni...   \n",
       "3                                                                                                         U dun say so early hor... U c already then say...   \n",
       "4                                                                                             Nah I don't think he goes to usf, he lives around here though   \n",
       "6                                                                             Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "10                                            I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.   \n",
       "14                                                                                                                      I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "16                                                                                                                               Oh k...i'm watching here:)   \n",
       "17                                                                        Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.   \n",
       "21                                                                                                          I‘m going to try for 2 months ha ha only joking   \n",
       "...                                                                                                                                                     ...   \n",
       "5554                                                     Well keep in mind I've only got enough gas for one more round trip barring a sudden influx of cash   \n",
       "5555  Yeh. Indians was nice. Tho it did kane me off a bit he he. We shud go out 4 a drink sometime soon. Mite hav 2 go 2 da works 4 a laugh soon. Love P...   \n",
       "5558                                                                                                                                 Sorry, I'll call later   \n",
       "5560                                                                                                                      Anything lor. Juz both of us lor.   \n",
       "5562                                                       Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering...   \n",
       "5563                                                                                                                                    Ard 6 like dat lor.   \n",
       "5565                                                                                                                                           Huh y lei...   \n",
       "5568                                                                                                                   Will ü b going to esplanade fr home?   \n",
       "5569                                                                                              Pity, * was in mood for that. So...any other suggestions?   \n",
       "5571                                                                                                                             Rofl. Its true to its name   \n",
       "\n",
       "      Cluster  \n",
       "0           6  \n",
       "1           6  \n",
       "3           6  \n",
       "4           6  \n",
       "6           6  \n",
       "10          6  \n",
       "14          6  \n",
       "16          6  \n",
       "17          6  \n",
       "21          6  \n",
       "...       ...  \n",
       "5554        6  \n",
       "5555        6  \n",
       "5558        6  \n",
       "5560        6  \n",
       "5562        6  \n",
       "5563        6  \n",
       "5565        6  \n",
       "5568        6  \n",
       "5569        6  \n",
       "5571        6  \n",
       "\n",
       "[3102 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_results[cluster_results.Cluster==6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 6 is 5.7% spam.  These look like fairly normal texts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the K-means cluster didn't produce clusters that provided much additional insight into the dataset, and it's  not nearly as useful as any of the models, which had far greater predicitive power.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
